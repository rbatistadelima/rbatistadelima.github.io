<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Face Detection Working Example</title>
  <style>
    body {
      margin: 0;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      background: #000;
      color: #0f0;
      font-family: sans-serif;
    }
    #container {
      position: relative;
      width: 640px;
      height: 480px;
    }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 640px;
      height: 480px;
      border-radius: 12px;
      object-fit: cover;
    }
    #label {
      position: absolute;
      bottom: 10px;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(0, 0, 0, 0.6);
      padding: 8px 16px;
      border-radius: 8px;
      font-weight: bold;
    }
  </style>
</head>
<body>
  <div id="container">
    <video id="video" autoplay muted playsinline></video>
    <canvas id="overlay"></canvas>
    <div id="label">Loading face model...</div>
  </div>

  <!-- face-api.js -->
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2"></script>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('overlay');
    const label = document.getElementById('label');
    const ctx = canvas.getContext('2d');

    // Load models from a working GitHub source
    const MODEL_URL = 'https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/';

    Promise.all([
      faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
      faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL)
    ])
    .then(startVideo)
    .catch(err => {
      label.textContent = 'Error loading models';
      console.error(err);
    });

    async function startVideo() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
        video.srcObject = stream;
        label.textContent = 'Detecting faces...';
      } catch (err) {
        label.textContent = 'Camera error: ' + err.message;
        console.error(err);
      }
    }

    video.addEventListener('play', () => {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;

      async function detect() {
        const detections = await faceapi
          .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
          .withFaceLandmarks();

        ctx.clearRect(0, 0, canvas.width, canvas.height);

        if (detections.length === 0) {
          label.textContent = 'No face detected';
        } else {
          label.textContent = `Faces detected: ${detections.length}`;
          detections.forEach(det => {
            const box = det.detection.box;
            ctx.strokeStyle = '#0f0';
            ctx.lineWidth = 2;
            ctx.strokeRect(box.x, box.y, box.width, box.height);

            det.landmarks.positions.forEach(pt => {
              ctx.beginPath();
              ctx.arc(pt.x, pt.y, 2, 0, Math.PI * 2);
              ctx.fillStyle = '#0f0';
              ctx.fill();
            });
          });
        }

        requestAnimationFrame(detect);
      }
      detect();
    });
  </script>
</body>
</html>
